# 联邦学习 通信挑战 概要技术论述

### 导言

随着移动互联网的普及和智能设备的迅速发展，**联邦学习（Federated Learning）** 作为一种分布式机器学习框架，逐渐成为解决数据隐私和通信效率问题的关键技术。与传统的集中式机器学习方法不同，联邦学习允许 **多个设备（如手机、物联网设备）在本地存储和处理数据**，而不是将数据上传到云端进行集中式训练。这种方式能够有效保护用户隐私，因为数据始终保留在本地设备上，避免了敏感信息的泄露。然而，联邦学习面临着一系列独特的挑战，其中 **通信效率** 是最为突出的一个问题。

在联邦学习中，客户端设备需要根据本地数据计算模型更新，并将这些更新传输到中央服务器进行聚合。这种 **客户端到服务器的频繁通信** 成为制约系统性能的瓶颈，尤其是在 **设备数量众多、网络连接不稳定或带宽有限的情况下**。每轮训练中，客户端需要传输的模型更新往往是高维的，且训练数据量和参与设备的规模都在不断增加，导致通信开销急剧上升。因此，如何优化联邦学习中的通信过程，减少每轮通信的数据量，成为实现高效、可扩展联邦学习系统的关键。

为了应对这一挑战，研究者们提出了多种策略来 **减轻通信负担**，如 **压缩模型更新**、**低秩表示** 或 **结构化更新**，这些方法通过减少传输的数据量来提高通信效率。尽管这些方法在一定程度上取得了成功，但如何在保证模型性能的同时，进一步优化通信过程，仍然是一个亟待解决的课题。

## 问题

### 同步与节点参与度不均问题：
联邦学习的全局模型更新参数会把一个周期内的局部模型的传递值采集完成才能进行更新。可以打破周期更新的局限性。局部模型参数可以随时上传，靠权重更新（异步通信）；

## 论文 

### [Federated Learning with Quantized Models](https://arxiv.org/abs/2003.03827)
该论文介绍了传递$H_t^{i}$的压缩方法: __Structured updates__ 和 __Sketched updates__ 

#### __Structured updates__
- 低秩矩阵: 限制$rank(H_t^{i}) ≤ k$
  $H_t^{i}= \mathbb{R}^{d_1 \times k}\cdot \mathbb{R}^{k \times d_2}$
- 随机掩码: 通过seed生成掩码矩阵，传输只用传递seed和非零值

#### __Sketched updates__
有损压缩
- 子采样: 与随机掩码类似，不同的是可能会发送0值
- 数据离散化: 
  这种方法可以把连续的数据划分到$2^{\mathbb{b}}$个离散值上，把数据用$\mathbb{b}$bit即可表示
  $$
  h_j =
  \begin{cases}
  h_{\text{max}}, & \text{with probability } \frac{h_j - h_{\text{min}}}{h_{\text{max}} - h_{\text{min}}} \\
  h_{\text{min}}, & \text{with probability } \frac{h_{\text{max}} - h_j}{h_{\text{max}} - h_{\text{min}}}
  \end{cases}
  $$
  但有局限性: 要求不同维度之间的数据范围大致在同一个数量级内

- 通过引入结构化的随机旋转来改进量化过程:
  通过旋转矩阵$R$,使$h' = Rh$的每个维度的数量级进行规范化。在全局更新阶段，需要进行逆旋转还原。需要注意的是，在实践中，$ h $ 的维度通常可以高达 $ d = 10^6 $ 或更高，而生成（$ O(d^3) $）和应用（$ O(d^2) $）一般的旋转矩阵在计算上是不可行的。与 Suresh 等人（2017）的做法相同，使用一种结构化的旋转矩阵，这种矩阵是 Walsh-Hadamard 矩阵与二进制对角矩阵的乘积。这将生成和应用矩阵的计算复杂度降低到 $ O(d) $ 和 $ O(d \log d) $，相对于联邦学习中的本地训练，这些复杂度是可以忽略不计的。

### [Robust and Communication-Efficient Federated Learning from Non-IID Data](https://arxiv.org/abs/2006.03899)
因为其他的文献大多在均分的情况下实现的上行压缩数据，不符合实际环境中非同分布的情况与要求下行压缩，该论文在非同分布角度在上下行通信问题上作出研究

在这项工作中，提出了稀疏三值压缩（STC）框架，这是一种专门为满足联邦学习环境需求而设计的新压缩框架。STC扩展了现有的压缩技术——top-k梯度稀疏化，采用了一种新的机制，使得能够进行下行压缩、三值量化处理以及权重更新的最优Golomb编码。

实验涵盖了四种不同的学习任务，结果表明，STC在常见的联邦学习场景中明显优于联邦平均（Federated Averaging），特别是在以下情况下：

- 客户端持有非独立同分布（non-iid）数据
- 客户端在训练过程中使用小批量（small batch size）
- 客户端数量庞大且每轮通信时的参与率较低。
- 即使客户端持有独立同分布（iid）数据并使用中等大小的批量进行训练，STC仍然在帕累托最优意义上优于联邦平均（Federated Averaging），即它在达到固定目标精度的情况下，所需的训练迭代次数较少，并且所需的通信预算也更小。这些结果支持在联邦优化中向高频、低位宽通信的范式转变，尤其是在带宽受限的学习环境中。

  $$
  b^{\text{up/down}} \in \mathcal{O}\left( \underbrace{N_{\text{iter}} \times f}_{\# \text{updates}} \times |\mathcal{W}| \times \underbrace{\left( H(\Delta \mathcal{W}^{\text{up/down}}) + \eta \right)}_{\text{update size}} \right)
  $$

以下是公式中参数的含义详细解释：

1. **$N_{\text{iter}}$**:
   - **含义**: 每个客户端执行的总训练迭代次数，包括前向传播和反向传播。
   - **重要性**: 它决定了模型训练的整体规模，迭代次数越多，训练越充分，但也会增加通信成本。
2. **$f$**:
   - **含义**: 通信频率，表示在训练过程中，客户端与服务器之间进行参数更新的频率。
   - 重要性:
     - 高通信频率（大的 f）可以更频繁地同步参数，可能提高模型的收敛速度，但会增加通信成本。
     - 低通信频率（小的 f）可以减少通信开销，但可能导致模型训练的性能下降。
   - 降低方法：
     从每轮传输，到隔100轮传输
3. **$|\mathcal{W}|$**:
   - **含义**: 模型参数的大小，即神经网络中所有权重的数量。
   - **重要性**: 模型越大（参数越多），通信开销越高。减少模型规模或使用更小的模型可以降低通信成本。
4. **$H(\Delta \mathcal{W}^{\text{up/down}})$**:
   - **含义**: 权重更新的熵，分别对应上传（up）和下载（down）过程中交换的权重更新量的熵值。
   - 重要性:
     - 熵是衡量更新信息量的指标，熵越高，说明需要传输的数据越多。
     - 通过有损压缩方法可以降低熵，从而减少通信量。
   - 降低方法：
     随机掩码，top-k，设立传输阀值
5. **$\eta$**:
   - **含义**: 编码效率的低效性，表示实际的更新大小与最小更新大小之间的差距（最小更新大小由熵 H 决定）。
   - 重要性:
     - 编码效率不高时，实际传输的数据会比理论最小值大，增加通信成本。
     - 使用更高效的编码方法可以减少 η 的值，从而降低通信开销。
   - 降低方法：
     离散值量化

#### 下行压缩

论文里面发现top-k的方法效果最好，因此从top-k上获得启发

- $$
  \Delta W(t+1) = \frac{1}{n} \sum_{i=1}^{n} \underbrace{ \text{top-k}\left( \Delta W^{(t+1)}_i + A^{(t)}_i \right)}_{ \tilde{\Delta W_i}^{(t+1)} }
  $$

- $$
  A_i^{t+1} = A_i^{t}+\Delta W_i^{t+1} -\tilde{\Delta W_i}^{(t+1)}
  $$

  

出现的问题：如果clinet的数量过多，这样更新会非常频繁，导致非零元素的数量过多，那么更新将不再是稀疏的，而变得接近密集。因此top-k捕获更新梯度的能力下降，收敛速度因此下降。



为了解决这个问题，论文的方法在中心server端也设置top-k方法传递$\Delta \tilde{W^{(t)}}$
- $$
  \Delta \tilde{W^{(t+1)}}=\text{top}_{p\%}(\frac{1}{n} \sum_{i=1}^{n}\underbrace{\text{top}_{p\%}(\Delta{W_i^{(t+1)}+A_i^{(t)}})}_{ \tilde{\Delta W_i}^{(t+1)} }+A^{(t)})
  $$


- $$
  A^{t+1} = A^{t}+\Delta W^{(t+1)} -\tilde{\Delta W}^{(t+1)} 
  $$

同时在以上的基础上对$\tilde{\Delta W_i^{(t+1)}}$和$\Delta \tilde{W^{(t+1)}}$近一步掩码压缩

- $$
  \tilde{\Delta W_i}^{(t+1)} = \text{top}_{p\%}((\Delta W_i^{(t+1)}+A_i^{(t)}) \odot M_i)
  $$

- $$
  \tilde{\Delta W}^{(t+1)} = \text{top}_{p\%}((\Delta W^{(t+1)}+A^{(t)}) \odot M)
  $$

  

#### 子节点权重更新缓存

由于各种网络硬件限制，子节点不会参与每次聚合，因此，sever端需要缓存前几轮的$\Delta W$
$$
\{ P^{(s)} = \sum_{t=1}^{s}\tilde{ \Delta W}^{(T-t)} | s = 1, \dots, \tau \}
$$
子节点如果要参加下一次更新，需要从server下载$P^{(s)}$或$W^{(T)}$，有一定局限性，因为各种网络硬件限制，子节点一般无法确定是否一定参与的了下一次全局更新。跳过的训练轮次越多，客户端下载的模型更新就越多，而这个数量与参与率成反比。由于下行链路比上行链路大，这种缓存开销是被允许的。

#### 消除冗余

##### 三值化

###### Algorithm 1: Sparse Ternary Compression (STC)

 - 1. **Input:** flattened tensor $ T \in \mathbb{R}^n $, sparsity $ p $
 - 2. **Output:** sparse ternary tensor $ T^* \in \{-\mu, 0, \mu\}^n $

$$
\begin{aligned}
&\text{Step 1: } k \leftarrow \max(np, 1) \\
&\text{Step 2: } v \leftarrow \text{topk}(\lvert T \rvert) \\
&\text{Step 3: } \text{mask} \leftarrow (\lvert T \rvert \geq v) \in \{0, 1\}^n \\
&\text{Step 4: } T_{\text{masked}} \leftarrow \text{mask} \cdot T \\
&\text{Step 5: } \mu \leftarrow \frac{1}{k} \sum_{i=1}^n \lvert T_{\text{masked}, i} \rvert \\
&\text{Step 6: } \text{Return: } T^* \leftarrow \mu \cdot \text{sign}(T_{\text{masked}})
\end{aligned}
$$



三值化使

$H_{\text{sparse}} = -p \log_2(p) - (1 - p) \log_2(1 - p) + 32p \tag{15}$减少到：$H_{\text{STC}} = -p \log_2(p) - (1 - p) \log_2(1 - p) + p \tag{16}$

局限性:之前提到过的数量级的问题

####  无损编码

##### Golomb编码

$$
b^* = 1 + \log_2 \left( \frac{\log(\phi - 1)}{\log(1 - p)} \right), \phi = \frac{\sqrt{5} + 1}{2}
$$

$$
\bar{b}_{\text{pos}} = b^* + \frac{1}{1 - (1 - p)^{2^{b^*}}}
$$



原理在于每个非零值之后再次出现非零值的概率为p，局限性为限制展平后的张量应该是呈几何分布的

1. **位置编码**：

    - 首先，计算每个非零元素在展平张量中的位置。这些位置通常是从零开始的索引值。

    - 然后，计算相邻非零元素之间的距离（即位置差）。这些距离是稀疏的，因此它们的分布可以近似为几何分布。

2. **应用 Golomb 编码**：

    - Golomb 编码的优点在于，它利用了距离分布的统计特性（即成功概率 $ p $），通过动态调整编码长度来达到最优压缩效果。具体地，**距离**（即两个非零元素之间的间隔）采用了一个参数 $ b^* $ 进行编码，其中：
​      1. $ b^* $ 是根据稀疏度 $ p $ 和其他参数计算得出的最优编码长度（公式中有具体的计算方式）。
​      2. 对每个间隔使用 Golomb 编码将位置差表示为较短的比特串，从而减少了位置编码的开销。

3. **位置和符号编码**：

    - 对于每个非零元素，我们不仅需要传输它的相对位置，还需要传输它的符号（即 $ \mu $ 或 $ -\mu $）。通常，通过额外的一个比特来表示符号。

4. **压缩效果**：
- 使用 Golomb 编码后，位置编码的平均位数（$ \bar{b}_{\text{pos}} $）将被压缩到 $ b^* + 1 $，这比起使用传统的固定长度编码（例如 16 位固定编码）要高效得多。例如，对于 $ p = 0.01 $ 的稀疏率，平均位数约为 $\bar{b}_{\text{pos}}=8.38$，比传统编码方案节省了约 1.9 倍的空间。

### [Federated Optimization in Heterogeneous Networks](https://arxiv.org/abs/1812.06127)

解决系统复杂性与异质性给联邦学习带来地挑战。在系统异质性的背景下，FedAvg不允许参与设备根据其底层系统约束执行不同数量的本地工作。通常的做法是简单地丢弃未能在指定时间窗口内计算E个周期的设备

**FedProx**（Federated Proximal）是一个用于 **联邦学习**（Federated Learning）中的优化方法，旨在解决传统联邦学习方法（如FedAvg）在异质数据和非独立同分布（non-IID）情况下性能不佳的问题。FedProx方法通过引入一个**proximal term**（近端项）来增强优化过程的鲁棒性，改善在数据分布不均的情况下的训练效果。




$$
\min_{\theta} \mathbb{E}_{i \sim P} \left[ \mathcal{L}_i(\theta) + \frac{\mu}{2} \|\theta - \theta_i^{\text{global}}\|^2 \right]
$$

其中：

- $\mathcal{L}_i(\theta)$ 是第 ii 个客户端的局部损失函数。
- $\theta_i^{\text{global}}$ 是全局模型的参数。
- $\mu$ 是正则化参数，控制proximal term的影响。
- $\|\theta - \theta_i^{\text{global}}\|$是每个客户端的局部模型和全局模型之间的差异。

我感觉只解决了数据复杂性带来的挑战，异质性并未解决。理想的模型应该是设备训练好了才上传，否则会损失部分信息

### [Efficient Federated Learning with Adaptive Communication and Local Update Policies](https://arxiv.org/abs/2405.03248)

这篇论文提出了一种在 **动态带宽** 环境下，利用 **自适应压缩** 技术来提高 **联邦学习** 中的 **通信效率** 的方法。传统的联邦学习在不同设备之间频繁传输大量模型更新，消耗了大量的带宽。在带宽动态变化的情况下，如何动态调整压缩率，以保证在有限的带宽条件下仍能有效地进行学习，是本文的研究重点。通过这种方式，可以有效减少通信开销，提高联邦学习的可扩展性和效率。
