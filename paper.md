# 联邦学习 通信挑战

### 导言

随着移动互联网的普及和智能设备的迅速发展，**联邦学习（Federated Learning）** 作为一种分布式机器学习框架，逐渐成为解决数据隐私和通信效率问题的关键技术。与传统的集中式机器学习方法不同，联邦学习允许 **多个设备（如手机、物联网设备）在本地存储和处理数据**，而不是将数据上传到云端进行集中式训练。这种方式能够有效保护用户隐私，因为数据始终保留在本地设备上，避免了敏感信息的泄露。然而，联邦学习面临着一系列独特的挑战，其中 **通信效率** 是最为突出的一个问题。

在联邦学习中，客户端设备需要根据本地数据计算模型更新，并将这些更新传输到中央服务器进行聚合。这种 **客户端到服务器的频繁通信** 成为制约系统性能的瓶颈，尤其是在 **设备数量众多、网络连接不稳定或带宽有限的情况下**。每轮训练中，客户端需要传输的模型更新往往是高维的，且训练数据量和参与设备的规模都在不断增加，导致通信开销急剧上升。因此，如何优化联邦学习中的通信过程，减少每轮通信的数据量，成为实现高效、可扩展联邦学习系统的关键。

为了应对这一挑战，研究者们提出了多种策略来 **减轻通信负担**，如 **压缩模型更新**、**低秩表示** 或 **结构化更新**，这些方法通过减少传输的数据量来提高通信效率。尽管这些方法在一定程度上取得了成功，但如何在保证模型性能的同时，进一步优化通信过程，仍然是一个亟待解决的课题。本篇论文旨在探讨当前联邦学习中通信面临的主要挑战，并提出一系列创新性的解决方案，以期在 **隐私保护** 和 **通信效率** 之间实现更好的平衡。

## 问题

### 同步与节点参与度不均问题：
联邦学习的全局模型更新参数会把一个周期内的局部模型的传递值采集完成才能进行更新。可以打破周期更新的局限性。局部模型参数可以随时上传，靠权重更新（异步通信）；

### 